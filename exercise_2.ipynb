{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# DATA20001 Deep Learning 2018 - Exercise 2\n",
    "\n",
    "**Due Wednesday November 21, before 23:59**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise 2.1. MNIST classifier with MLP in numpy\n",
    "\n",
    "For this exercise, we will implement a classifier for the MNIST handwritten digit dataset. This dataset consists of grayscale images of handwritten numbers of size 28x28 pixels (with values between 0 and 255) and labels between 0 and 9. You can use the following code to download the dataset and load it into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import zlib\n",
    "import struct as st\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_data_url = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
    "train_labels_url = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\n",
    "test_data_url = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\n",
    "test_labels_url = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "bsize = 1048576\n",
    "\n",
    "def download(url):\n",
    "    print(f'Downloading {url}...')\n",
    "    r = requests.get(url, stream=True)\n",
    "    size = int(r.headers['content-length']) // bsize\n",
    "    # Download in-memory\n",
    "    buff = io.BytesIO()\n",
    "    pbar = tqdm(\n",
    "        r.iter_content(bsize),\n",
    "        total = size,\n",
    "        unit = 'MiB',\n",
    "        unit_scale = True,\n",
    "        unit_divisor = 1024,\n",
    "        bar_format = '{l_bar}{bar}| [{elapsed}/{remaining}, {rate_fmt}]'\n",
    "    )\n",
    "    dec = zlib.decompressobj(32 + zlib.MAX_WBITS)\n",
    "    for chunk in pbar:\n",
    "        data = dec.decompress(chunk)\n",
    "        buff.write(data)\n",
    "    buff.seek(0)\n",
    "    return buff\n",
    "\n",
    "def parse_idx(url, idx3=False):\n",
    "    buff = download(url)\n",
    "    print('Parsing binary data...')\n",
    "    magic = st.unpack('>4B', buff.read(4))\n",
    "    n = st.unpack('>I', buff.read(4))[0]\n",
    "    if idx3:\n",
    "        rows = st.unpack('>I', buff.read(4))[0]\n",
    "        cols = st.unpack('>I', buff.read(4))[0]\n",
    "        total = n * rows * cols\n",
    "        shape = (n, rows, cols)\n",
    "    else:\n",
    "        total = n\n",
    "        shape = n\n",
    "    arr = np.asarray(\n",
    "        st.unpack(\n",
    "            f'>{total}B',\n",
    "            buff.read(total)\n",
    "        )\n",
    "    ).reshape(shape)\n",
    "    buff.close()\n",
    "    if idx3:\n",
    "        arr = 255 - arr\n",
    "    return arr\n",
    "\n",
    "train_data = parse_idx(train_data_url, idx3=True)\n",
    "train_labels = parse_idx(train_labels_url)\n",
    "test_data = parse_idx(test_data_url, idx3=True)\n",
    "test_labels = parse_idx(test_labels_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The classifier should be an MLP with the following architecture:\n",
    "* 1 hidden layer with 50 neurons and ReLU activation.\n",
    "* 1 output layer with 10 neurons and Softmax activation.\n",
    "\n",
    "The ReLU function returns the positive part of an input, or 0 if it is negative:\n",
    "$$ReLU(x) = \\begin{cases} 0 & \\text{if } x < 0 \\\\ x & x \\geq 0 \\end{cases}$$\n",
    "\n",
    "Its derivative is given by the Heaviside step function:\n",
    "\n",
    "$$H(x) = \\begin{cases} 0 & \\text{if } x < 0 \\\\ 1 & x \\geq 0 \\end{cases}$$\n",
    "\n",
    "The Softmax function converts an arbitrary set of K numbers into a probability distribution, by normalizing them between 0 and 1 and making them add up to 1:\n",
    "\n",
    "$$\\sigma(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$$\n",
    "\n",
    "It is a generalization of the logistic function we saw previously, and so its derivative is similar:\n",
    "\n",
    "$$ \\frac{\\partial \\sigma(\\mathbf{z})_j}{\\partial  z_i} = \\begin{cases} \\sigma(\\mathbf{z})_i(1 - \\sigma(\\mathbf{z})_i) & i = j \\\\ -\\sigma(\\mathbf{z})_i \\sigma(\\mathbf{z})_j & i \\neq j \\end{cases}$$\n",
    "\n",
    "The output of the network will be the probability that the input corresponds to each of the 10 classes. The true label will be given by the probability distribution where the probability of the true class is 1 and the probability of all other classes is 0, i.e. the 1-hot vector of the class index.\n",
    "\n",
    "Therefore, we will use cross-entropy as the loss function, which gives a measure of how close two probability distributions p and q are:\n",
    "\n",
    "$$H(p, q) = -\\sum_x p(x)\\, \\log q(x)$$\n",
    "\n",
    "Since our true distribution always has a single 1 for the true class t and the rest of the values are 0, the expression for cross-entropy loss for the predicted probability is simplified to:\n",
    "\n",
    "$$L(\\hat{p}_t) = - \\log \\hat{p}_t$$\n",
    "\n",
    "The derivative for this expression is thus:\n",
    "\n",
    "$$ \\frac{d L(\\hat{p}_t)}{d p_t} = -p_t^{-1}$$\n",
    "\n",
    "The value of L is 0 for all other classes that are not t, so we don't backpropagate the gradients for those.\n",
    "\n",
    "You can add more layers and/or change the number of neurons in your hidden layer if you wish. You should train with the training set only, and you can use the test set to verify whether your network is learning properly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise 2.2. MLP with pytorch\n",
    "\n",
    "The task is to create an MLP to classify images using pytorch (see e.g. the tutorial from lecture 4).  Here we'll use the FashionMNIST dataset that contains images of ten different classes of clothing.\n",
    "\n",
    "Below are some commands to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Let's load the dataset, fortunately FashionMNIST is also available directly in torchvision\n",
    "batch_size = 32\n",
    "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "validation_dataset = datasets.FashionMNIST('./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "There are 10 classes as in the regular MNIST, but each class represent an item of clothing, below we defined a map from the class index to the textual label.  This is just for humans to look at :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "labels = {\n",
    "  0: 'T-shirt/top',\n",
    "  1: 'Trouser',\n",
    "  2: 'Pullover',\n",
    "  3: 'Dress',\n",
    "  4: 'Coat',\n",
    "  5: 'Sandal',\n",
    "  6: 'Shirt',\n",
    "  7: 'Sneaker',\n",
    "  8: 'Bag',\n",
    "  9: 'Ankle boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's take a look at the first ten images in the training set just to get an idea of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break\n",
    "\n",
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray_r\")\n",
    "    plt.title(labels[y_train[i].item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now, <span style=\"background-color: yellow\">your task is to train an MLP model, i.e., a neural network with several fully-connected layers to classify images into the ten classes</span>.  You should train on the training set loaded above, and you should use the validation set to calculate the accuracy of the model (i.e., the percentage of correctly classified images of the validation set).\n",
    "\n",
    "You can use many layers, and any non-linearities you wish.  You should get an accuracy at least above 85%.\n",
    "\n",
    "<span style=\"background-color: yellow\">Please also plot the training loss versus the validation loss across the epochs (i.e., epoch number on the x-axis, and the two loss curves on the y-axis). Also discuss the difference in the two curves.</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "name": "exercise_2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
